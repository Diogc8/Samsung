{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwZoVUueNg9W"
      },
      "source": [
        "# Exercise Notebook Part 1\n",
        "\n",
        "**Practice exercises for Part 1** - Follows **Learning Notebook Part 1**\n",
        "\n",
        "This notebook covers **Exercises 1-8** (foundational NLP preprocessing and basic search):\n",
        "\n",
        "1. **Exercise 1**: Text preprocessing with regex (URLs, emails, etc.) + sub-exercises (1a-1d)\n",
        "2. **Exercise 2**: Tokenization techniques comparison + sub-exercises (2a-2e)\n",
        "3. **Exercise 3**: Term Frequency (TF) calculation / Bag of Words (BoW)\n",
        "4. **Exercise 4**: TF-based keyword search\n",
        "5. **Exercise 5**: Compare preprocessing approaches\n",
        "6. **Exercise 6**: Stemming and Lemmatization + sub-exercises (6a-6d)\n",
        "7. **Exercise 7**: Advanced regex patterns + sub-exercises (7a-7d)\n",
        "8. **Exercise 8**: Handling special cases in preprocessing + sub-exercises (8a-8e)\n",
        "\n",
        "**üìù Note**: Exercise Notebook Part 2 will cover more advanced topics including TF-IDF, similarity search, document clustering, and RAG (Retrieval-Augmented Generation).\n",
        "\n",
        "**Important Pipeline Order:**\n",
        "```\n",
        "Preprocessing ‚Üí Tokenization ‚Üí Vectorization (BoW/TF) ‚Üí Keyword Search\n",
        "```\n",
        "\n",
        "**Instructions**: Complete each exercise by filling in the code cells marked with `# TODO`\n",
        "\n",
        "**Note**:\n",
        "- **TF-IDF** (IDF calculation and full TF-IDF) is in **Exercise Notebook Part 2**\n",
        "- **Similarity search** and **RAG** are in **Exercise Notebook Part 2**\n",
        "- Remember that TF-IDF is **syntactic** (word-based, no meaning). True semantic search (understanding meaning, synonyms) requires embeddings (Class 3)! **Semantic = meaning**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hrUZjU0YNg9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "9T-trRizNg9c",
        "outputId": "d39ff03b-0eaa-4a3a-9fa4-acf353ad56ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data file not found. Downloading from GitHub...\n",
            "‚úì Data file downloaded successfully!\n",
            "Loaded 10000 movies\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movie_id                title  \\\n",
              "0         1         Edge of Code   \n",
              "1         2      Storm of Secret   \n",
              "2         3  Under Warrior Redux   \n",
              "3         4      Quest of Secret   \n",
              "4         5          Key of Game   \n",
              "\n",
              "                                         description      genre  rating  \n",
              "0  A compelling romance film about a young advent...    Romance     7.1  \n",
              "1  This captivating romance movie follows a quest...    Romance     6.3  \n",
              "2  In this captivating war story, a secret organi...        War     7.3  \n",
              "3  A compelling fantasy film about a determined d...    Fantasy     8.3  \n",
              "4  A exploration adventure film about a master th...  Adventure     6.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11087493-343c-4e7c-a295-fe811e07bc67\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>genre</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Edge of Code</td>\n",
              "      <td>A compelling romance film about a young advent...</td>\n",
              "      <td>Romance</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Storm of Secret</td>\n",
              "      <td>This captivating romance movie follows a quest...</td>\n",
              "      <td>Romance</td>\n",
              "      <td>6.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Under Warrior Redux</td>\n",
              "      <td>In this captivating war story, a secret organi...</td>\n",
              "      <td>War</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Quest of Secret</td>\n",
              "      <td>A compelling fantasy film about a determined d...</td>\n",
              "      <td>Fantasy</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Key of Game</td>\n",
              "      <td>A exploration adventure film about a master th...</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11087493-343c-4e7c-a295-fe811e07bc67')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11087493-343c-4e7c-a295-fe811e07bc67 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11087493-343c-4e7c-a295-fe811e07bc67');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9fd331a6-2a5d-4ebb-9c65-d372b196fd42\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9fd331a6-2a5d-4ebb-9c65-d372b196fd42')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9fd331a6-2a5d-4ebb-9c65-d372b196fd42 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"movie_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 1,\n        \"max\": 10000,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          6253,\n          4685,\n          1732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9983,\n        \"samples\": [\n          \"Agent of Hero\",\n          \"Battle Saga\",\n          \"Secret of Circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9136,\n        \"samples\": [\n          \"A engaging crime film about a secret organization. the film explores themes of courage and redemption.\",\n          \"A captivating western film about a forbidden love. featuring outstanding performances and breathtaking cinematography.\",\n          \"This thrilling action movie follows a small town. full of unexpected twists and turns. features spectacular action sequences and stunts.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"Romance\",\n          \"Sport\",\n          \"Western\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2023180604448267,\n        \"min\": 1.8,\n        \"max\": 10.0,\n        \"num_unique_values\": 79,\n        \"samples\": [\n          4.7,\n          7.1,\n          6.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Load movie data\n",
        "# If running in Google Colab and data file doesn't exist, download it from GitHub\n",
        "import os\n",
        "\n",
        "if not os.path.exists('data/movies.csv'):\n",
        "    print(\"Data file not found. Downloading from GitHub...\")\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    import urllib.request\n",
        "    url = 'https://raw.githubusercontent.com/samsung-ai-course/8th-9th-edition/main/Chapter%202%20-%20Natural%20Language%20Processing/Class%201%20%26%202%20-%20NLP%20and%20Search/data/movies.csv'\n",
        "    urllib.request.urlretrieve(url, 'data/movies.csv')\n",
        "    print(\"‚úì Data file downloaded successfully!\")\n",
        "\n",
        "df = pd.read_csv('data/movies.csv')\n",
        "print(f\"Loaded {len(df)} movies\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbhUADeBNg9d"
      },
      "source": [
        "## Exercise 1: Text Cleaning with Regex\n",
        "\n",
        "Complete the `clean_text` function to:\n",
        "1. Remove URLs (starting with http:// or https://)\n",
        "2. Remove email addresses\n",
        "3. Remove phone numbers (format: (555) 123-4567 or 555-123-4567)\n",
        "4. Remove extra whitespace\n",
        "5. Convert to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean Text Function ---\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing URLs, emails, phone numbers, extra whitespace,\n",
        "    and converting to lowercase.\n",
        "    \"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove emails\n",
        "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n",
        "\n",
        "    # Remove phone numbers ((555) 123-4567 or 555-123-4567)\n",
        "    text = re.sub(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply cleaning to description column\n",
        "df['clean_description'] = df['description'].apply(clean_text)\n",
        "df[['description', 'clean_description']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SZhlanUOXom-",
        "outputId": "bfb6f5b0-2f73-428d-8034-3fb6ac7cf153"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         description  \\\n",
              "0  A compelling romance film about a young advent...   \n",
              "1  This captivating romance movie follows a quest...   \n",
              "2  In this captivating war story, a secret organi...   \n",
              "3  A compelling fantasy film about a determined d...   \n",
              "4  A exploration adventure film about a master th...   \n",
              "\n",
              "                                   clean_description  \n",
              "0  a compelling romance film about a young advent...  \n",
              "1  this captivating romance movie follows a quest...  \n",
              "2  in this captivating war story, a secret organi...  \n",
              "3  a compelling fantasy film about a determined d...  \n",
              "4  a exploration adventure film about a master th...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-926086da-e02d-4e4a-a8ea-c6a7afbc720c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>clean_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A compelling romance film about a young advent...</td>\n",
              "      <td>a compelling romance film about a young advent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This captivating romance movie follows a quest...</td>\n",
              "      <td>this captivating romance movie follows a quest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this captivating war story, a secret organi...</td>\n",
              "      <td>in this captivating war story, a secret organi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A compelling fantasy film about a determined d...</td>\n",
              "      <td>a compelling fantasy film about a determined d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A exploration adventure film about a master th...</td>\n",
              "      <td>a exploration adventure film about a master th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-926086da-e02d-4e4a-a8ea-c6a7afbc720c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-926086da-e02d-4e4a-a8ea-c6a7afbc720c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-926086da-e02d-4e4a-a8ea-c6a7afbc720c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c8ed196b-2507-4ec0-a70c-35fef3f959d4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8ed196b-2507-4ec0-a70c-35fef3f959d4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c8ed196b-2507-4ec0-a70c-35fef3f959d4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[['description', 'clean_description']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"This captivating romance movie follows a quest for truth. the film explores themes of courage and redemption. a touching love story that will warm your heart.\",\n          \"A exploration adventure film about a master thief. an epic adventure that spans continents and generations.\",\n          \"In this captivating war story, a secret organization. the film explores themes of courage and redemption.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"this captivating romance movie follows a quest for truth. the film explores themes of courage and redemption. a touching love story that will warm your heart.\",\n          \"a exploration adventure film about a master thief. an epic adventure that spans continents and generations.\",\n          \"in this captivating war story, a secret organization. the film explores themes of courage and redemption.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vge3F6HtNg9e"
      },
      "source": [
        "### Exercise 1a: Extract Specific Patterns from Text\n",
        "\n",
        "Instead of removing patterns, sometimes we want to **extract** them for analysis.\n",
        "Complete functions to extract dates, prices, hashtags and anything else you might think its relevant.\n",
        "\n",
        "**Goal**: Practice pattern extraction and understand when to extract vs remove.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_dates(text):\n",
        "    # Matches formats like 2025-11-12, 11/12/2025, Nov 12 2025\n",
        "    return re.findall(r'\\b(?:\\d{4}-\\d{2}-\\d{2}|\\d{1,2}/\\d{1,2}/\\d{4}|[A-Z][a-z]{2,8}\\s\\d{1,2}\\s\\d{4})\\b', text)\n",
        "\n",
        "def extract_prices(text):\n",
        "    # Matches $12.99, ‚Ç¨10, 10 USD\n",
        "    return re.findall(r'[$‚Ç¨¬£]\\d+(?:\\.\\d{2})?|\\d+\\s?(?:USD|EUR|GBP)', text)\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    return re.findall(r'#\\w+', text)\n",
        "\n",
        "def extract_mentions(text):\n",
        "    return re.findall(r'@\\w+', text)\n",
        "\n",
        "# Example usage\n",
        "sample = \"New movie releases on Nov 12 2025! Tickets cost ‚Ç¨10. #Cinema @MovieFan\"\n",
        "print(\"Dates:\", extract_dates(sample))\n",
        "print(\"Prices:\", extract_prices(sample))\n",
        "print(\"Hashtags:\", extract_hashtags(sample))\n",
        "print(\"Mentions:\", extract_mentions(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDmBRhVDbVKo",
        "outputId": "6032eca5-64d8-4451-e817-edc69236122e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates: ['Nov 12 2025']\n",
            "Prices: ['‚Ç¨10']\n",
            "Hashtags: ['#Cinema']\n",
            "Mentions: ['@MovieFan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi_iOuQ8Ng9e"
      },
      "source": [
        "### Exercise 1b: Normalize Text Content\n",
        "\n",
        "Sometimes we want to **normalize** text (standardize variations) rather than remove it.\n",
        "Complete functions to normalize contractions, abbreviations, and special characters.\n",
        "\n",
        "**Goal**: Understand when normalization improves text consistency for NLP.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Text Normalization?\n",
        "\n",
        "**Normalization** in NLP means converting different variations of text into a **standard, consistent form** while preserving the meaning. Unlike **removal** (which deletes content), normalization **transforms** text to reduce variation and improve consistency.\n",
        "\n",
        "### Why Normalize?\n",
        "\n",
        "Text often contains multiple ways to express the same thing:\n",
        "- **Contractions**: \"don't\" vs \"do not\", \"I'm\" vs \"I am\", \"can't\" vs \"cannot\"\n",
        "- **Abbreviations**: \"Dr.\" vs \"Doctor\", \"U.S.A.\" vs \"USA\" vs \"United States\"\n",
        "- **Special characters**: \"caf√©\" vs \"cafe\", \"na√Øve\" vs \"naive\", \"r√©sum√©\" vs \"resume\"\n",
        "- **Punctuation variations**: \"Mr.\" vs \"Mr\", \"e.g.\" vs \"eg\"\n",
        "- **Number formats**: \"1,000\" vs \"1000\", \"50%\" vs \"50 percent\"\n",
        "\n",
        "### Normalization vs Removal\n",
        "\n",
        "| Approach | Example | When to Use |\n",
        "|----------|---------|-------------|\n",
        "| **Normalization** | \"don't\" ‚Üí \"do not\" | Preserve meaning, reduce vocabulary size |\n",
        "| **Removal** | Remove URLs, emails | Content is noise, not useful for analysis |\n",
        "\n",
        "### Benefits of Normalization\n",
        "\n",
        "1. **Reduces Vocabulary Size**: \"don't\", \"don't\", \"do not\", \"do not\" ‚Üí all become \"do not\"\n",
        "2. **Improves Matching**: Search for \"cannot\" will also match \"can't\"\n",
        "3. **Consistency**: Same concept represented the same way across documents\n",
        "4. **Better Statistics**: TF-IDF counts are more accurate when variations are unified\n",
        "\n",
        "### Examples of Normalization\n",
        "\n",
        "```python\n",
        "# Contractions\n",
        "\"don't\" ‚Üí \"do not\"\n",
        "\"I'm\" ‚Üí \"I am\"\n",
        "\"won't\" ‚Üí \"will not\"\n",
        "\"it's\" ‚Üí \"it is\" (or \"it has\" depending on context)\n",
        "\n",
        "# Abbreviations\n",
        "\"Dr. Smith\" ‚Üí \"Doctor Smith\"\n",
        "\"U.S.A.\" ‚Üí \"USA\" (or \"United States of America\")\n",
        "\"e.g.\" ‚Üí \"for example\"\n",
        "\"i.e.\" ‚Üí \"that is\"\n",
        "\n",
        "# Special Characters\n",
        "\"caf√©\" ‚Üí \"cafe\"\n",
        "\"na√Øve\" ‚Üí \"naive\"\n",
        "\"r√©sum√©\" ‚Üí \"resume\"\n",
        "\n",
        "# Punctuation\n",
        "\"Mr.\" ‚Üí \"Mr\"\n",
        "\"Mrs.\" ‚Üí \"Mrs\"\n",
        "\"Ph.D.\" ‚Üí \"PhD\"\n",
        "```\n",
        "\n",
        "### When NOT to Normalize\n",
        "\n",
        "- **Proper nouns**: \"U.S.A.\" (country) vs \"USA\" (abbreviation) - context matters\n",
        "- **Domain-specific terms**: \"AI\" vs \"artificial intelligence\" - may have different meanings\n",
        "- **Sentiment analysis**: \"don't\" vs \"do not\" - contractions can carry different emotional weight\n",
        "- **Preserving original format**: When exact text matching is required\n",
        "\n",
        "### Implementation Strategy\n",
        "\n",
        "1. **Contractions**: Use a dictionary mapping contractions to full forms\n",
        "2. **Abbreviations**: Create abbreviation dictionaries (context-dependent)\n",
        "3. **Special Characters**: Use Unicode normalization (NFD/NFC) or character mapping\n",
        "4. **Punctuation**: Remove or standardize punctuation marks consistently\n",
        "\n",
        "**Key Insight**: Normalization is a trade-off between consistency and information preservation. Choose normalization strategies based on your specific NLP task!\n",
        "\n",
        "---\n",
        "\n",
        "### üí° PS: Useful Frameworks for Normalization\n",
        "\n",
        "Instead of manually creating endless dictionaries for contractions, abbreviations, and special characters, consider using established NLP frameworks:\n",
        "\n",
        "- **spaCy**: Provides built-in text normalization, lemmatization, and tokenization\n",
        "  ```python\n",
        "  import spacy\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(\"I don't think it's working\")\n",
        "  # Access normalized tokens, lemmas, etc.\n",
        "  ```\n",
        "\n",
        "- **NLTK**: Offers contraction expansion, word normalization, and various text processing utilities\n",
        "  ```python\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  from nltk.corpus import stopwords\n",
        "  ```\n",
        "\n",
        "- **TextBlob**: Simple API for common NLP tasks including normalization\n",
        "  ```python\n",
        "  from textblob import TextBlob\n",
        "  blob = TextBlob(\"I don't like it\")\n",
        "  ```\n",
        "\n",
        "- **Unidecode**: Specifically for Unicode normalization (removing accents, special characters)\n",
        "  ```python\n",
        "  from unidecode import unidecode\n",
        "  unidecode(\"caf√©\")  # Returns \"cafe\"\n",
        "  ```\n",
        "\n",
        "- **contractions**: Python library specifically for expanding contractions\n",
        "  ```python\n",
        "  import contractions\n",
        "  contractions.fix(\"don't\")  # Returns \"do not\"\n",
        "  ```\n",
        "\n",
        "**Note**: While these frameworks are helpful, understanding the underlying concepts (as you'll practice in this exercise) is crucial for customizing normalization for your specific use case!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "# 1Ô∏è‚É£ Contractions normalization\n",
        "contractions_dict = {\n",
        "    \"don't\": \"do not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"let's\": \"let us\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "# 2Ô∏è‚É£ Abbreviation normalization\n",
        "abbrev_dict = {\n",
        "    \"u.s.a.\": \"usa\",\n",
        "    \"dr.\": \"doctor\",\n",
        "    \"e.g.\": \"for example\",\n",
        "    \"i.e.\": \"that is\"\n",
        "}\n",
        "\n",
        "def normalize_abbreviations(text):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(abbrev_dict.keys()) + r')\\b', re.IGNORECASE)\n",
        "    return pattern.sub(lambda x: abbrev_dict[x.group().lower()], text)\n",
        "\n",
        "# 3Ô∏è‚É£ Remove special characters (normalize Unicode)\n",
        "def normalize_special_chars(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "# Combine all\n",
        "def normalize_text(text):\n",
        "    text = expand_contractions(text)\n",
        "    text = normalize_abbreviations(text)\n",
        "    text = normalize_special_chars(text)\n",
        "    return text\n",
        "\n",
        "example = \"I‚Äôm Dr. Smith and I don‚Äôt like caf√© culture in the U.S.A.\"\n",
        "print(normalize_text(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iZkfCprbYLz",
        "outputId": "946e02d7-61fc-430f-cb53-79f53b518068"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Im Dr. Smith and I dont like cafe culture in the U.S.A.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sk1tgfnNg9g"
      },
      "source": [
        "### Exercise 1c: Clean HTML and Markdown\n",
        "\n",
        "Real-world text often contains HTML tags or markdown formatting.\n",
        "Complete functions to remove HTML tags while preserving text content, and clean markdown.\n",
        "\n",
        "**Goal**: Handle structured text formats commonly found in web content.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def clean_html(text):\n",
        "    \"\"\"Remove HTML tags and keep text content.\"\"\"\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
        "\n",
        "def clean_markdown(text):\n",
        "    \"\"\"Remove markdown formatting like **bold**, _italic_, [links](url), etc.\"\"\"\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)  # [text](url)\n",
        "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)        # bold\n",
        "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)           # italic\n",
        "    text = re.sub(r'`{1,3}.*?`{1,3}', '', text)            # code blocks\n",
        "    text = re.sub(r'#+\\s?', '', text)                      # headers\n",
        "    return text.strip()\n",
        "\n",
        "html_sample = \"<p>This is a <b>bold</b> paragraph with <a href='url'>link</a>.</p>\"\n",
        "md_sample = \"This is **bold**, _italic_, and a [link](https://example.com).\"\n",
        "\n",
        "print(\"HTML cleaned:\", clean_html(html_sample))\n",
        "print(\"Markdown cleaned:\", clean_markdown(md_sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B85Cw1Tkbj7I",
        "outputId": "59f0b9a8-6446-47a7-ee5b-187fb803d79f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML cleaned: This is a  bold  paragraph with  link .\n",
            "Markdown cleaned: This is bold, italic, and a link.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnC-NrFINg9i"
      },
      "source": [
        "### Exercise 1d: Compare Cleaning Strategies\n",
        "\n",
        "Compare the impact of different cleaning approaches on vocabulary size and text quality.\n",
        "This helps understand when to apply different cleaning techniques.\n",
        "\n",
        "**Goal**: Measure the practical impact of preprocessing choices.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove URLs, emails, phone numbers, extra whitespace, and lowercase text.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)                    # URLs\n",
        "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)                      # Emails\n",
        "    text = re.sub(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '', text)  # Phones\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                      # Extra spaces\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Exercise 1b helpers: Normalization\n",
        "# ============================================================\n",
        "contractions_dict = {\n",
        "    \"don't\": \"do not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"let's\": \"let us\"\n",
        "}\n",
        "\n",
        "def normalize_contractions(text):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "\n",
        "abbrev_dict = {\n",
        "    \"u.s.a.\": \"usa\",\n",
        "    \"dr.\": \"doctor\",\n",
        "    \"e.g.\": \"for example\",\n",
        "    \"i.e.\": \"that is\"\n",
        "}\n",
        "\n",
        "def normalize_abbreviations(text):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(abbrev_dict.keys()) + r')\\b', re.IGNORECASE)\n",
        "    return pattern.sub(lambda x: abbrev_dict[x.group().lower()], text)"
      ],
      "metadata": {
        "id": "XWKbAaCocbZe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k62jSr1TNg9i",
        "outputId": "229b2916-e10a-4291-e6a0-c012c170048f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Cleaning Strategy Comparison\n",
            "======================================================================\n",
            "Strategy                  Vocab Size      Total Tokens    Avg Length     \n",
            "----------------------------------------------------------------------\n",
            "No Cleaning               80              172             17.20          \n",
            "Basic Cleaning            77              172             17.20          \n",
            "Full Cleaning             77              172             17.20          \n",
            "Full + Normalization      77              172             17.20          \n",
            "\n",
            "======================================================================\n",
            "Key Insights:\n",
            "======================================================================\n",
            "Vocabulary reduction: 80 ‚Üí 77\n",
            "Reduction percentage: 3.7%\n",
            "\n",
            "When to use each strategy:\n",
            "  - No cleaning: Preserve original text for exact matching\n",
            "  - Basic cleaning: Simple normalization, fast processing\n",
            "  - Full cleaning: Remove noise, reduce vocabulary size\n",
            "  - Full + Normalization: Maximum consistency, best for NLP tasks\n"
          ]
        }
      ],
      "source": [
        "def compare_cleaning_strategies(df, column='description', sample_size=10):\n",
        "    \"\"\"\n",
        "    Compare different cleaning strategies and measure their impact.\n",
        "\n",
        "    Strategies to compare:\n",
        "    1. No cleaning (baseline)\n",
        "    2. Basic cleaning (lowercase, whitespace)\n",
        "    3. Full cleaning (remove URLs, emails, etc.)\n",
        "    4. Full cleaning + normalization (contractions, abbreviations)\n",
        "\n",
        "    Metrics to measure:\n",
        "    - Vocabulary size (unique words)\n",
        "    - Average document length\n",
        "    - Number of tokens\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    sample_texts = df[column].head(sample_size).tolist()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Strategy 1: No cleaning\n",
        "    vocab_no_clean = set()\n",
        "    total_tokens_no_clean = 0\n",
        "    for text in sample_texts:\n",
        "        tokens = text.split()\n",
        "        vocab_no_clean.update(tokens)\n",
        "        total_tokens_no_clean += len(tokens)\n",
        "\n",
        "    results['No Cleaning'] = {\n",
        "        'vocab_size': len(vocab_no_clean),\n",
        "        'total_tokens': total_tokens_no_clean,\n",
        "        'avg_length': total_tokens_no_clean / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Strategy 2: Basic cleaning (lowercase, whitespace)\n",
        "    vocab_basic = set()\n",
        "    total_tokens_basic = 0\n",
        "    for text in sample_texts:\n",
        "        # TODO: Apply basic cleaning (lowercase, normalize whitespace)\n",
        "        cleaned = text.lower()\n",
        "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "        tokens = cleaned.split()\n",
        "        vocab_basic.update(tokens)\n",
        "        total_tokens_basic += len(tokens)\n",
        "\n",
        "    results['Basic Cleaning'] = {\n",
        "        'vocab_size': len(vocab_basic),\n",
        "        'total_tokens': total_tokens_basic,\n",
        "        'avg_length': total_tokens_basic / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Strategy 3: Full cleaning (use your clean_text function)\n",
        "    vocab_full = set()\n",
        "    total_tokens_full = 0\n",
        "    for text in sample_texts:\n",
        "        # TODO: Apply full cleaning (remove URLs, emails, etc.)\n",
        "        cleaned = clean_text(text)  # Use your function from Exercise 1\n",
        "        tokens = cleaned.split()\n",
        "        vocab_full.update(tokens)\n",
        "        total_tokens_full += len(tokens)\n",
        "\n",
        "    results['Full Cleaning'] = {\n",
        "        'vocab_size': len(vocab_full),\n",
        "        'total_tokens': total_tokens_full,\n",
        "        'avg_length': total_tokens_full / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Strategy 4: Full cleaning + normalization\n",
        "    vocab_norm = set()\n",
        "    total_tokens_norm = 0\n",
        "    for text in sample_texts:\n",
        "        # TODO: Apply full cleaning + normalization\n",
        "        cleaned = clean_text(text)\n",
        "        cleaned = normalize_contractions(cleaned)\n",
        "        cleaned = normalize_abbreviations(cleaned)\n",
        "        tokens = cleaned.split()\n",
        "        vocab_norm.update(tokens)\n",
        "        total_tokens_norm += len(tokens)\n",
        "\n",
        "    results['Full + Normalization'] = {\n",
        "        'vocab_size': len(vocab_norm),\n",
        "        'total_tokens': total_tokens_norm,\n",
        "        'avg_length': total_tokens_norm / len(sample_texts)\n",
        "    }\n",
        "\n",
        "    # Display comparison\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Cleaning Strategy Comparison\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"{'Strategy':<25} {'Vocab Size':<15} {'Total Tokens':<15} {'Avg Length':<15}\")\n",
        "    print(\"-\" * 70)\n",
        "    for strategy, metrics in results.items():\n",
        "        print(f\"{strategy:<25} {metrics['vocab_size']:<15} {metrics['total_tokens']:<15} {metrics['avg_length']:<15.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Key Insights:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Vocabulary reduction: {results['No Cleaning']['vocab_size']} ‚Üí {results['Full + Normalization']['vocab_size']}\")\n",
        "    print(f\"Reduction percentage: {(1 - results['Full + Normalization']['vocab_size'] / results['No Cleaning']['vocab_size']) * 100:.1f}%\")\n",
        "    print(\"\\nWhen to use each strategy:\")\n",
        "    print(\"  - No cleaning: Preserve original text for exact matching\")\n",
        "    print(\"  - Basic cleaning: Simple normalization, fast processing\")\n",
        "    print(\"  - Full cleaning: Remove noise, reduce vocabulary size\")\n",
        "    print(\"  - Full + Normalization: Maximum consistency, best for NLP tasks\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run comparison\n",
        "comparison_results = compare_cleaning_strategies(df, sample_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ovFcnQTNg9k"
      },
      "source": [
        "## Exercise 2: Tokenization\n",
        "\n",
        "Complete the `tokenize_text` function to:\n",
        "1. Tokenize text into words\n",
        "2. Filter out very short tokens (length < 3)\n",
        "3. Optionally filter stop words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "# ============================================================\n",
        "# Exercise 2: Tokenize Text\n",
        "# ============================================================\n",
        "def tokenize_text(text, min_length=3, remove_stopwords=True):\n",
        "    \"\"\"\n",
        "    Tokenizes text into words, filters short tokens, and optionally removes stop words.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # Basic tokenization using regex\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())  # split on word boundaries\n",
        "\n",
        "    # Filter by min_length\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "\n",
        "    # Optionally remove stop words\n",
        "    if remove_stopwords:\n",
        "        tokens = [t for t in tokens if t not in STOP_WORDS]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example\n",
        "sample = \"This is a simple example: it shows how tokenization works.\"\n",
        "print(tokenize_text(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHAhbA_1cssD",
        "outputId": "ee160630-5233-4631-d036-b6e9abdb0e32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'example', 'shows', 'tokenization', 'works']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkwiRp5HNg9l"
      },
      "source": [
        "### Exercise 2a: Compare Tokenization Methods\n",
        "\n",
        "Compare different tokenization approaches to understand their impact.\n",
        "Different methods split text differently, affecting vocabulary size and token quality.\n",
        "\n",
        "**Goal**: Understand trade-offs between different tokenization techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def compare_tokenization_methods(text):\n",
        "    \"\"\"\n",
        "    Compare regex-based, NLTK, and spaCy tokenization outputs.\n",
        "    \"\"\"\n",
        "    nltk_tokens = nltk.word_tokenize(text)\n",
        "    regex_tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    spacy_tokens = [token.text for token in nlp(text)]\n",
        "\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"\\n--- Regex Tokenization ---\")\n",
        "    print(regex_tokens)\n",
        "    print(\"\\n--- NLTK Tokenization ---\")\n",
        "    print(nltk_tokens)\n",
        "    print(\"\\n--- spaCy Tokenization ---\")\n",
        "    print(spacy_tokens)\n",
        "\n",
        "    print(f\"\\nVocabulary sizes:\")\n",
        "    print(f\"Regex: {len(set(regex_tokens))}, NLTK: {len(set(nltk_tokens))}, spaCy: {len(set(spacy_tokens))}\")\n",
        "\n",
        "# Example\n",
        "compare_tokenization_methods(\"Mr. Smith‚Äôs caf√© is open 24/7 in the U.S.A.!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BJfET2Ac194",
        "outputId": "185fb767-3ad4-4938-a6a6-d02316183751"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Mr. Smith‚Äôs caf√© is open 24/7 in the U.S.A.!\n",
            "\n",
            "--- Regex Tokenization ---\n",
            "['mr', 'smith', 's', 'caf√©', 'is', 'open', '24', '7', 'in', 'the', 'u', 's', 'a']\n",
            "\n",
            "--- NLTK Tokenization ---\n",
            "['Mr.', 'Smith', '‚Äô', 's', 'caf√©', 'is', 'open', '24/7', 'in', 'the', 'U.S.A.', '!']\n",
            "\n",
            "--- spaCy Tokenization ---\n",
            "['Mr.', 'Smith', '‚Äôs', 'caf√©', 'is', 'open', '24/7', 'in', 'the', 'U.S.A.', '!']\n",
            "\n",
            "Vocabulary sizes:\n",
            "Regex: 12, NLTK: 12, spaCy: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxNU2fw7Ng9l"
      },
      "source": [
        "### Exercise 2b: Test Impact of min_length Threshold\n",
        "\n",
        "Test how different minimum token length thresholds affect vocabulary size and token quality.\n",
        "Shorter tokens (like \"a\", \"I\", \"it\") are often less informative but may be important in some contexts.\n",
        "\n",
        "**Goal**: Understand the trade-off between vocabulary size and token informativeness.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_min_length_impact(texts, lengths=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Test how min_length affects vocabulary size and token count.\n",
        "    \"\"\"\n",
        "    for L in lengths:\n",
        "        all_tokens = [token for text in texts for token in tokenize_text(text, min_length=L)]\n",
        "        vocab_size = len(set(all_tokens))\n",
        "        print(f\"min_length={L}: {len(all_tokens)} tokens, vocab size={vocab_size}\")\n",
        "\n",
        "# Example\n",
        "test_min_length_impact(df['description'].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-w3Vd4XdNd_",
        "outputId": "43747393-80c8-4629-a563-59773e6534e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min_length=1: 61 tokens, vocab size=38\n",
            "min_length=2: 61 tokens, vocab size=38\n",
            "min_length=3: 61 tokens, vocab size=38\n",
            "min_length=4: 60 tokens, vocab size=37\n",
            "min_length=5: 48 tokens, vocab size=32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vHxVirVNg9l"
      },
      "source": [
        "### Exercise 2c: Test Impact of Stop Word Removal\n",
        "\n",
        "Compare tokenization with and without stop word removal.\n",
        "Stop words are common but may be important for some tasks (e.g., sentiment analysis).\n",
        "\n",
        "**Goal**: Understand when stop word removal helps vs. hurts NLP tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_stopword_removal(texts):\n",
        "    \"\"\"\n",
        "    Compare tokenization with and without stop word removal.\n",
        "    \"\"\"\n",
        "    all_tokens_no_stop = [t for text in texts for t in tokenize_text(text, remove_stopwords=False)]\n",
        "    all_tokens_stop = [t for text in texts for t in tokenize_text(text, remove_stopwords=True)]\n",
        "\n",
        "    print(f\"With stop words: {len(set(all_tokens_no_stop))} unique tokens\")\n",
        "    print(f\"Without stop words: {len(set(all_tokens_stop))} unique tokens\")\n",
        "    print(f\"Reduction: {(1 - len(set(all_tokens_stop))/len(set(all_tokens_no_stop)))*100:.1f}%\")\n",
        "\n",
        "# Example\n",
        "compare_stopword_removal(df['description'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQffuf7mdS5y",
        "outputId": "2f508835-f709-4576-ab31-90f2429b9932"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With stop words: 71 unique tokens\n",
            "Without stop words: 62 unique tokens\n",
            "Reduction: 12.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3go4fPcNg9m"
      },
      "source": [
        "### Exercise 2d: Tokenize Different Text Types\n",
        "\n",
        "Compare tokenization on different types of text (titles vs descriptions).\n",
        "Different text types have different characteristics and may need different preprocessing.\n",
        "\n",
        "**Goal**: Understand how text type affects tokenization decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_text_types(df):\n",
        "    \"\"\"\n",
        "    Compare tokenization characteristics between movie titles and descriptions.\n",
        "    \"\"\"\n",
        "    title_tokens = [t for text in df['title'] for t in tokenize_text(text)]\n",
        "    desc_tokens = [t for text in df['description'] for t in tokenize_text(text)]\n",
        "\n",
        "    print(f\"Titles: {len(set(title_tokens))} unique tokens, avg length {len(title_tokens)/len(df):.1f}\")\n",
        "    print(f\"Descriptions: {len(set(desc_tokens))} unique tokens, avg length {len(desc_tokens)/len(df):.1f}\")\n",
        "\n",
        "    print(f\"\\nVocabulary overlap: {len(set(title_tokens) & set(desc_tokens))} shared words\")\n",
        "\n",
        "# Example\n",
        "compare_text_types(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V3Z3QuzdWTq",
        "outputId": "898843ef-7cd9-4c63-cbee-1c09141374ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titles: 73 unique tokens, avg length 2.0\n",
            "Descriptions: 169 unique tokens, avg length 11.7\n",
            "\n",
            "Vocabulary overlap: 13 shared words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNU1aVw-Ng9m"
      },
      "source": [
        "### Exercise 2e: Create N-grams from Tokens\n",
        "\n",
        "After tokenization, create n-grams (unigrams, bigrams, trigrams) to capture word order.\n",
        "Compare vocabulary size and sparsity of different n-gram approaches.\n",
        "\n",
        "**Goal**: Understand how n-grams capture context and affect vocabulary size.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def create_ngrams(tokens, n=2):\n",
        "    \"\"\"\n",
        "    Create n-grams (bigrams, trigrams, etc.) from a list of tokens.\n",
        "    \"\"\"\n",
        "    return ['_'.join(gram) for gram in ngrams(tokens, n)]\n",
        "\n",
        "def compare_ngrams(texts, n_values=[1, 2, 3]):\n",
        "    \"\"\"\n",
        "    Compare vocabulary size and sparsity across different n-gram models.\n",
        "    \"\"\"\n",
        "    for n in n_values:\n",
        "        all_ngrams = []\n",
        "        for text in texts:\n",
        "            tokens = tokenize_text(text)\n",
        "            grams = create_ngrams(tokens, n) if n > 1 else tokens\n",
        "            all_ngrams.extend(grams)\n",
        "        vocab_size = len(set(all_ngrams))\n",
        "        print(f\"{n}-grams: {len(all_ngrams)} total, {vocab_size} unique\")\n",
        "\n",
        "# Example\n",
        "compare_ngrams(df['description'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p424cynqdawJ",
        "outputId": "81e3a3d0-a1e8-4713-b0f8-394670525ca4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams: 111 total, 62 unique\n",
            "2-grams: 101 total, 78 unique\n",
            "3-grams: 91 total, 75 unique\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Y95zr4Ng9m"
      },
      "source": [
        "## Exercise 3: Bag of Words (BoW) / Term Frequency (TF) Calculation\n",
        "\n",
        "Implement Term Frequency (TF) calculation manually - this is also called Bag of Words (BoW)!\n",
        "\n",
        "**Note**: This is Part 1 of vectorization. In **Exercise Notebook Part 2**, you'll learn IDF and full TF-IDF calculation.\n",
        "\n",
        "**What you'll implement:**\n",
        "1. Calculate Term Frequency (TF) - count how many times a word appears in a document\n",
        "2. TF = count(term) / total_terms_in_document\n",
        "3. This creates Bag of Words vectors - simple word counts!\n",
        "\n",
        "**Key Point**: BoW/TF is just counting words - it's the foundation before we learn TF-IDF in Part 2!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OE7ISVUNg9n",
        "outputId": "3e41c687-5066-463a-86d7-e84fdd2b741d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['deep', 'language', 'learning', 'machine', 'natural', 'processing']\n",
            "\n",
            "TF of 'natural' in doc 0: 0.3333333333333333\n",
            "TF of 'language' in doc 0: 0.3333333333333333\n",
            "\n",
            "Bag of Words vectors:\n",
            "Doc 0: ['natural', 'language', 'processing'] ‚Üí [0, 1, 0, 0, 1, 1]\n",
            "Doc 1: ['machine', 'learning', 'natural'] ‚Üí [0, 0, 1, 1, 1, 0]\n",
            "Doc 2: ['deep', 'learning', 'language'] ‚Üí [1, 1, 1, 0, 0, 0]\n",
            "\n",
            "üí° Note: These are simple word counts (BoW). In Part 2, you'll learn TF-IDF!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_tf(term, document_tokens):\n",
        "    \"\"\"\n",
        "    Calculate Term Frequency: count(term) / total_terms\n",
        "    \"\"\"\n",
        "    # Conta quantas vezes o termo aparece no documento\n",
        "    term_count = document_tokens.count(term)\n",
        "\n",
        "    # Divide pelo n√∫mero total de palavras no documento\n",
        "    total_terms = len(document_tokens)\n",
        "\n",
        "    # Evita divis√£o por zero\n",
        "    if total_terms == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return term_count / total_terms\n",
        "\n",
        "\n",
        "def create_bow_vector(document_tokens, vocabulary):\n",
        "    \"\"\"\n",
        "    Create a Bag of Words vector for a document.\n",
        "    Each position corresponds to the count of that word in the document.\n",
        "    \"\"\"\n",
        "    # Conta quantas vezes cada token aparece\n",
        "    token_counts = Counter(document_tokens)\n",
        "\n",
        "    # Cria o vetor onde cada posi√ß√£o corresponde √† contagem de uma palavra do vocabul√°rio\n",
        "    bow_vector = [token_counts[word] for word in vocabulary]\n",
        "\n",
        "    return bow_vector\n",
        "\n",
        "\n",
        "# Test with simple example\n",
        "docs = [\n",
        "    [\"natural\", \"language\", \"processing\"],\n",
        "    [\"machine\", \"learning\", \"natural\"],\n",
        "    [\"deep\", \"learning\", \"language\"]\n",
        "]\n",
        "\n",
        "# Build vocabulary\n",
        "all_words = set()\n",
        "for doc in docs:\n",
        "    all_words.update(doc)\n",
        "vocab = sorted(list(all_words))\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"\\nTF of 'natural' in doc 0:\", calculate_tf(\"natural\", docs[0]))\n",
        "print(\"TF of 'language' in doc 0:\", calculate_tf(\"language\", docs[0]))\n",
        "\n",
        "print(\"\\nBag of Words vectors:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    bow_vector = create_bow_vector(doc, vocab)\n",
        "    print(f\"Doc {i}: {doc} ‚Üí {bow_vector}\")\n",
        "\n",
        "print(\"\\nüí° Note: These are simple word counts (BoW). In Part 2, you'll learn TF-IDF!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SZevwnyNg9n"
      },
      "source": [
        "## Exercise 4: TF-Based Keyword Search\n",
        "\n",
        "Implement a keyword search that ranks results by Term Frequency (TF).\n",
        "For multiple query words, combine their TF scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_by_tf(query, documents, vocab):\n",
        "    \"\"\"\n",
        "    Rank documents by the sum of Term Frequencies (TF) of query terms.\n",
        "    Args:\n",
        "        query (str): search query (e.g., \"machine learning\")\n",
        "        documents (list[list[str]]): list of tokenized documents\n",
        "        vocab (list[str]): full vocabulary (sorted)\n",
        "    Returns:\n",
        "        list of tuples: (doc_index, score) sorted by descending score\n",
        "    \"\"\"\n",
        "    query_terms = query.lower().split()\n",
        "    results = []\n",
        "\n",
        "    for i, doc_tokens in enumerate(documents):\n",
        "        score = 0\n",
        "        for term in query_terms:\n",
        "            score += calculate_tf(term, doc_tokens)\n",
        "        results.append((i, score))\n",
        "\n",
        "    # Sort by score descending\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "    return results\n",
        "\n",
        "\n",
        "# üîπ Example\n",
        "query = \"natural learning\"\n",
        "results = search_by_tf(query, docs, vocab)\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "for idx, score in results:\n",
        "    print(f\"Doc {idx}: {docs[idx]} ‚Üí TF Score = {score:.3f}\")"
      ],
      "metadata": {
        "id": "jzrns5-xk8JB",
        "outputId": "b73a26fd-c51e-438b-af52-cb7a80be469c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: 'natural learning'\n",
            "\n",
            "Doc 1: ['machine', 'learning', 'natural'] ‚Üí TF Score = 0.667\n",
            "Doc 0: ['natural', 'language', 'processing'] ‚Üí TF Score = 0.333\n",
            "Doc 2: ['deep', 'learning', 'language'] ‚Üí TF Score = 0.333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoKQ7ljUNg9o"
      },
      "source": [
        "## Exercise 5: Compare Preprocessing Approaches\n",
        "\n",
        "Compare search results with and without preprocessing (stop words removal, lowercasing).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text, remove_stopwords=True, lowercase=True):\n",
        "    tokens = text.split()\n",
        "    if lowercase:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words('english'))\n",
        "        tokens = [t for t in tokens if t not in stops]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# üîπ Example comparison\n",
        "text = \"The Natural Language Processing course is amazing!\"\n",
        "print(\"Original tokens:\", text.split())\n",
        "print(\"Preprocessed:\", preprocess_text(text))"
      ],
      "metadata": {
        "id": "QigNfsYfk-O2",
        "outputId": "f40fc349-860b-4667-c9ce-d962a44e39c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['The', 'Natural', 'Language', 'Processing', 'course', 'is', 'amazing!']\n",
            "Preprocessed: ['natural', 'language', 'processing', 'course', 'amazing!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKsphsJGNg9o"
      },
      "source": [
        "## Exercise 6: Stemming and Lemmatization\n",
        "\n",
        "Stemming and lemmatization are advanced preprocessing techniques that reduce words to their root forms:\n",
        "- **Stemming**: Quick, rule-based reduction (e.g., \"running\" ‚Üí \"run\", \"better\" ‚Üí \"better\")\n",
        "- **Lemmatization**: More accurate, context-aware reduction using dictionaries (e.g., \"better\" ‚Üí \"good\")\n",
        "\n",
        "**When to use**: Useful for reducing vocabulary size and handling word variations (running/ran/run).\n",
        "\n",
        "Complete exercises below to understand both approaches!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLAjGBF7Ng9o"
      },
      "source": [
        "### Exercise 6a: Implement Stemming\n",
        "\n",
        "Complete the `apply_stemming` function using NLTK's PorterStemmer or SnowballStemmer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def apply_stemming(tokens):\n",
        "    \"\"\"\n",
        "    Apply stemming using PorterStemmer.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "\n",
        "# üîπ Example\n",
        "tokens = [\"running\", \"runs\", \"easily\", \"fairness\"]\n",
        "print(\"Before stemming:\", tokens)\n",
        "print(\"After stemming:\", apply_stemming(tokens))"
      ],
      "metadata": {
        "id": "pjE7GjlHlB5t",
        "outputId": "92f1d8ab-bea6-4184-9b01-769c52320c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before stemming: ['running', 'runs', 'easily', 'fairness']\n",
            "After stemming: ['run', 'run', 'easili', 'fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wap6wGmJNg9p"
      },
      "source": [
        "### Exercise 6b: Implement Lemmatization\n",
        "\n",
        "Complete the `apply_lemmatization` function using NLTK's WordNetLemmatizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def apply_lemmatization(tokens):\n",
        "    \"\"\"\n",
        "    Apply lemmatization using WordNetLemmatizer.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "\n",
        "# üîπ Example\n",
        "tokens = [\"running\", \"better\", \"feet\"]\n",
        "print(\"Before lemmatization:\", tokens)\n",
        "print(\"After lemmatization:\", apply_lemmatization(tokens))"
      ],
      "metadata": {
        "id": "4U1G_CaYlLLx",
        "outputId": "a32162d2-6d53-4e60-9429-87c34ddfccbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before lemmatization: ['running', 'better', 'feet']\n",
            "After lemmatization: ['running', 'better', 'foot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJujdUJxNg9p"
      },
      "source": [
        "### Exercise 6c: Compare Stemming vs Lemmatization\n",
        "\n",
        "Compare the results of stemming and lemmatization on the same text.\n",
        "Analyze when each approach is better.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"running\", \"runs\", \"better\", \"feet\", \"studies\", \"studying\"]\n",
        "\n",
        "print(\"\\nOriginal:\", tokens)\n",
        "print(\"Stemmed:\", apply_stemming(tokens))\n",
        "print(\"Lemmatized:\", apply_lemmatization(tokens))"
      ],
      "metadata": {
        "id": "QrMS6buXlQ6o",
        "outputId": "77710a0d-c96b-46ec-894c-7c799b8dc031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original: ['running', 'runs', 'better', 'feet', 'studies', 'studying']\n",
            "Stemmed: ['run', 'run', 'better', 'feet', 'studi', 'studi']\n",
            "Lemmatized: ['running', 'run', 'better', 'foot', 'study', 'studying']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SCy7k8MNg9p"
      },
      "source": [
        "### Exercise 6d: Complete Preprocessing Pipeline with Stemming/Lemmatization\n",
        "\n",
        "Create a complete preprocessing function that includes optional stemming or lemmatization.\n",
        "Compare search results with and without stemming/lemmatization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gUQe_BtNg9q",
        "outputId": "4c9cf585-2215-4eb6-b11f-291541b9ba8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# garantir recursos necess√°rios\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def preprocess_text_complete(text,\n",
        "                             remove_stop_words=True,\n",
        "                             min_length=3,\n",
        "                             stem=False,\n",
        "                             lemmatize=False):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline with optional stemming/lemmatization.\n",
        "\n",
        "    Args:\n",
        "        text: Raw text input\n",
        "        remove_stop_words: Whether to remove stop words\n",
        "        min_length: Minimum token length\n",
        "        stem: If True, apply stemming\n",
        "        lemmatize: If True, apply lemmatization (overrides stem if both True)\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed tokens\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Clean text (use your clean_text function from Exercise 1 if defined)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)                 # keep only letters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()             # normalize spaces\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 3: Remove stop words\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # Step 4: Filter by length\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "\n",
        "    # Step 5: Apply stemming or lemmatization\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    elif stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on a movie description\n",
        "sample_desc = df.loc[0, 'description']\n",
        "print(\"Original description:\", sample_desc[:200])\n",
        "\n",
        "# Test different preprocessing combinations\n",
        "tokens_basic = preprocess_text_complete(sample_desc, stem=False, lemmatize=False)\n",
        "tokens_stemmed = preprocess_text_complete(sample_desc, stem=True, lemmatize=False)\n",
        "tokens_lemmatized = preprocess_text_complete(sample_desc, stem=False, lemmatize=True)\n",
        "\n",
        "print(\"\\nBasic tokens:\", tokens_basic[:20])\n",
        "print(\"\\nStemmed tokens:\", tokens_stemmed[:20])\n",
        "print(\"\\nLemmatized tokens:\", tokens_lemmatized[:20])\n",
        "\n",
        "print(\"\\nVocabulary sizes:\")\n",
        "print(\"Basic:\", len(set(tokens_basic)))\n",
        "print(\"Stemmed:\", len(set(tokens_stemmed)))\n",
        "print(\"Lemmatized:\", len(set(tokens_lemmatized)))"
      ],
      "metadata": {
        "id": "n78HHkEil0I5",
        "outputId": "1045c7d3-c48c-4491-8890-6b39dd8ce48b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original description: A compelling romance film about a young adventurer. an epic adventure that spans continents and generations. a touching love story that will warm your heart.\n",
            "\n",
            "Basic tokens: ['compelling', 'romance', 'film', 'young', 'adventurer', 'epic', 'adventure', 'spans', 'continents', 'generations', 'touching', 'love', 'story', 'warm', 'heart']\n",
            "\n",
            "Stemmed tokens: ['compel', 'romanc', 'film', 'young', 'adventur', 'epic', 'adventur', 'span', 'contin', 'gener', 'touch', 'love', 'stori', 'warm', 'heart']\n",
            "\n",
            "Lemmatized tokens: ['compelling', 'romance', 'film', 'young', 'adventurer', 'epic', 'adventure', 'span', 'continent', 'generation', 'touching', 'love', 'story', 'warm', 'heart']\n",
            "\n",
            "Vocabulary sizes:\n",
            "Basic: 15\n",
            "Stemmed: 14\n",
            "Lemmatized: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IK4XXsiNg9q"
      },
      "source": [
        "## Summary: What You've Practiced in Part 1\n",
        "\n",
        "‚úÖ **Exercise 1**: Text cleaning with regex (URLs, emails, phone numbers)  \n",
        "  - **1a**: Extract specific patterns (dates, prices, hashtags, mentions)  \n",
        "  - **1b**: Normalize text content (contractions, abbreviations, special chars)  \n",
        "  - **1c**: Clean HTML and Markdown  \n",
        "  - **1d**: Compare cleaning strategies  \n",
        "\n",
        "‚úÖ **Exercise 2**: Tokenization with stop word removal  \n",
        "  - **2a**: Compare tokenization methods (split, regex, word boundaries)  \n",
        "  - **2b**: Test impact of min_length threshold  \n",
        "  - **2c**: Test impact of stop word removal  \n",
        "  - **2d**: Tokenize different text types (titles vs descriptions)  \n",
        "  - **2e**: Create n-grams from tokens  \n",
        "\n",
        "‚úÖ **Exercise 3**: Bag of Words (BoW) / Term Frequency (TF) calculation  \n",
        "‚úÖ **Exercise 4**: TF-based keyword search  \n",
        "‚úÖ **Exercise 5**: Compare preprocessing approaches  \n",
        "‚úÖ **Exercise 6**: Stemming and Lemmatization (6a, 6b, 6c, 6d)  \n",
        "‚úÖ **Exercise 7**: Advanced regex patterns  \n",
        "‚úÖ **Exercise 8**: Handling special cases in preprocessing  \n",
        "\n",
        "**Key Takeaways:**\n",
        "- Preprocessing quality directly affects search results\n",
        "- Different tokenization methods have different trade-offs (speed vs. accuracy vs. context)\n",
        "- min_length and stop word removal significantly impact vocabulary size\n",
        "- Text type matters: titles vs descriptions need different approaches\n",
        "- N-grams capture word order but increase vocabulary size and sparsity\n",
        "- Stemming vs Lemmatization: Choose based on speed vs accuracy needs\n",
        "- Regex is powerful for cleaning but handle edge cases carefully\n",
        "- Bag of Words (BoW/TF) is simple word counting - foundation for TF-IDF!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz45w8K7Ng9r"
      },
      "source": [
        "## Exercise 7: Advanced Regex Patterns\n",
        "\n",
        "Practice more complex regex patterns for text preprocessing.\n",
        "\n",
        "**Goal**: Master advanced regex techniques commonly used in NLP preprocessing.\n",
        "\n",
        "**What you'll practice:**\n",
        "1. Named groups and non-capturing groups\n",
        "2. Lookahead and lookbehind assertions\n",
        "3. Complex pattern matching (dates, currencies, numbers)\n",
        "4. Regex substitution with callbacks\n",
        "5. Handling unicode and special characters\n",
        "\n",
        "**Why this matters**: Real-world text contains complex patterns that require sophisticated regex to extract or clean properly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyeok3TBNg9r"
      },
      "source": [
        "# Exercise 7a: Extract Dates in Different Formats\n",
        "\n",
        "def extract_dates(text):\n",
        "    \"\"\"\n",
        "    Extract dates in various formats:\n",
        "    - \"January 1, 2024\" or \"Jan 1, 2024\"\n",
        "    - \"2024-01-01\" or \"01/01/2024\"\n",
        "    - \"1st January 2024\"\n",
        "    \n",
        "    Use named groups to extract day, month, year separately.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text containing dates\n",
        "    \n",
        "    Returns:\n",
        "        list: List of tuples (day, month, year) or dicts with named groups\n",
        "    \"\"\"\n",
        "    # TODO: Create regex pattern(s) to match different date formats\n",
        "    # TODO: Use named groups (?P<name>pattern) to extract components\n",
        "    # TODO: Return list of matches\n",
        "    \n",
        "    dates = []\n",
        "    # Hint: Use re.finditer() or re.findall() with named groups\n",
        "    \n",
        "    return dates\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie was released on January 15, 2024.\n",
        "    It premiered on 2024-03-20 in theaters.\n",
        "    The sequel came out on 04/15/2024.\n",
        "    The original was on 1st January 2020.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Extract dates and print results\n",
        "# dates = extract_dates(test_text)\n",
        "# for date in dates:\n",
        "#     print(date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhO8h-01Ng9s"
      },
      "outputs": [],
      "source": [
        "# Exercise 7b: Extract Currency and Numbers\n",
        "\n",
        "def extract_currency(text):\n",
        "    \"\"\"\n",
        "    Extract currency amounts in different formats:\n",
        "    - \"$100\", \"$1,000.50\", \"$1M\", \"$1.5B\"\n",
        "    - \"‚Ç¨50\", \"¬£200\", \"¬•1000\"\n",
        "    - \"100 dollars\", \"fifty euros\"\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        list: List of currency amounts with their symbols/units\n",
        "    \"\"\"\n",
        "    # TODO: Create regex patterns to match currency formats\n",
        "    # TODO: Extract amount and currency symbol/unit\n",
        "    # TODO: Handle different currency symbols ($, ‚Ç¨, ¬£, ¬•)\n",
        "    # TODO: Handle abbreviations (M = million, B = billion, K = thousand)\n",
        "\n",
        "    currencies = []\n",
        "\n",
        "    return currencies\n",
        "\n",
        "def extract_numbers(text):\n",
        "    \"\"\"\n",
        "    Extract numbers in various formats:\n",
        "    - Integers: \"100\", \"1,000\", \"1 million\"\n",
        "    - Decimals: \"3.14\", \"1,234.56\"\n",
        "    - Percentages: \"50%\", \"25.5 percent\"\n",
        "    - Ordinals: \"1st\", \"2nd\", \"3rd\", \"4th\"\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with keys 'integers', 'decimals', 'percentages', 'ordinals'\n",
        "    \"\"\"\n",
        "    # TODO: Extract different number types\n",
        "    # TODO: Use named groups or separate patterns for each type\n",
        "\n",
        "    numbers = {\n",
        "        'integers': [],\n",
        "        'decimals': [],\n",
        "        'percentages': [],\n",
        "        'ordinals': []\n",
        "    }\n",
        "\n",
        "    return numbers\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie grossed $150 million at the box office.\n",
        "    It cost $50M to produce and made ‚Ç¨75.5M worldwide.\n",
        "    The rating was 8.5/10 with 95% positive reviews.\n",
        "    It ranked 1st in its opening weekend, 2nd overall.\n",
        "    The budget was approximately 1.5 billion dollars.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Extract currencies and numbers\n",
        "# currencies = extract_currency(test_text)\n",
        "# numbers = extract_numbers(test_text)\n",
        "# print(\"Currencies:\", currencies)\n",
        "# print(\"Numbers:\", numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLA6QdnDNg9s"
      },
      "outputs": [],
      "source": [
        "# Exercise 7c: Lookahead and Lookbehind Patterns\n",
        "\n",
        "def extract_quoted_text(text):\n",
        "    \"\"\"\n",
        "    Extract text within quotes, handling nested quotes.\n",
        "    Use lookahead/lookbehind to ensure proper matching.\n",
        "\n",
        "    Args:\n",
        "        text: Input text with quoted strings\n",
        "\n",
        "    Returns:\n",
        "        list: List of quoted text (without the quotes)\n",
        "    \"\"\"\n",
        "    # TODO: Use positive lookbehind (?<=...) and positive lookahead (?=...)\n",
        "    # TODO: Match text between quotes (single or double)\n",
        "    # TODO: Handle escaped quotes inside strings\n",
        "\n",
        "    quoted = []\n",
        "\n",
        "    return quoted\n",
        "\n",
        "def extract_context_words(text, target_word, context_size=3):\n",
        "    \"\"\"\n",
        "    Extract words around a target word using lookahead/lookbehind.\n",
        "\n",
        "    Example: For \"machine learning\" in \"I love machine learning and AI\",\n",
        "             extract \"love\", \"and\", \"AI\" (context_size=1)\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        target_word: Word to find context for\n",
        "        context_size: Number of words before and after to extract\n",
        "\n",
        "    Returns:\n",
        "        dict: {'before': [words], 'after': [words], 'target': word}\n",
        "    \"\"\"\n",
        "    # TODO: Use lookbehind to capture words before target\n",
        "    # TODO: Use lookahead to capture words after target\n",
        "    # TODO: Return context words\n",
        "\n",
        "    context = {\n",
        "        'before': [],\n",
        "        'after': [],\n",
        "        'target': target_word\n",
        "    }\n",
        "\n",
        "    return context\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The director said \"This is the best movie I've ever made.\"\n",
        "    He added, 'It's a masterpiece' and everyone agreed.\n",
        "    The phrase \"machine learning\" appears often in AI discussions.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Extract quoted text and context\n",
        "# quoted = extract_quoted_text(test_text)\n",
        "# context = extract_context_words(test_text, \"machine\", context_size=2)\n",
        "# print(\"Quoted text:\", quoted)\n",
        "# print(\"Context:\", context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTF96eoWNg9t"
      },
      "outputs": [],
      "source": [
        "# Exercise 7d: Regex Substitution with Callbacks\n",
        "\n",
        "def anonymize_emails(text):\n",
        "    \"\"\"\n",
        "    Replace email addresses with \"[EMAIL]\" placeholder.\n",
        "    Use re.sub() with a function callback.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Text with emails anonymized\n",
        "    \"\"\"\n",
        "    # TODO: Use re.sub() with a function that replaces email pattern\n",
        "    # Pattern: something@domain.com\n",
        "    # Replace with: \"[EMAIL]\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def normalize_whitespace_advanced(text):\n",
        "    \"\"\"\n",
        "    Normalize whitespace, but preserve intentional line breaks.\n",
        "    - Replace multiple spaces with single space\n",
        "    - Replace multiple newlines (2+) with double newline (paragraph break)\n",
        "    - Preserve single newlines (line breaks)\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    # TODO: Use re.sub() with callbacks to handle different whitespace patterns\n",
        "    # TODO: Preserve intentional formatting while cleaning up excessive whitespace\n",
        "\n",
        "    return text\n",
        "\n",
        "def format_numbers_readable(text):\n",
        "    \"\"\"\n",
        "    Format large numbers to be more readable.\n",
        "    - \"1000000\" ‚Üí \"1,000,000\"\n",
        "    - \"1500\" ‚Üí \"1,500\"\n",
        "    - But preserve decimals: \"1234.56\" ‚Üí \"1,234.56\"\n",
        "\n",
        "    Use re.sub() with a callback function to format numbers.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Text with formatted numbers\n",
        "    \"\"\"\n",
        "    # TODO: Find numbers in text\n",
        "    # TODO: Use callback function to add commas every 3 digits\n",
        "    # TODO: Preserve decimal points\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    Contact us at info@example.com or support@company.org.\n",
        "    The movie made 1500000 dollars in its first week.\n",
        "    It had 5000 viewers on opening day.\n",
        "    The budget was 2500000.50 dollars.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test anonymization and formatting\n",
        "# anonymized = anonymize_emails(test_text)\n",
        "# formatted = format_numbers_readable(test_text)\n",
        "# print(\"Anonymized:\", anonymized)\n",
        "# print(\"Formatted:\", formatted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7p1tmUiNg9u"
      },
      "source": [
        "## Exercise 8: Handling Special Cases in Preprocessing\n",
        "\n",
        "Handle edge cases in text preprocessing that are common in real-world data.\n",
        "\n",
        "**Goal**: Make preprocessing robust to handle messy, real-world text data.\n",
        "\n",
        "**What you'll handle:**\n",
        "1. Mixed encoding issues (unicode, emojis, special characters)\n",
        "2. Inconsistent capitalization (acronyms, proper nouns)\n",
        "3. Numbers and units (measurements, percentages, dates)\n",
        "4. Abbreviations and contractions\n",
        "5. Whitespace inconsistencies and formatting artifacts\n",
        "6. Missing or corrupted data (NaN, None, empty strings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZfXijajNg9v"
      },
      "outputs": [],
      "source": [
        "# Exercise 6a: Handle Unicode and Special Characters\n",
        "\n",
        "def clean_unicode_text(text):\n",
        "    \"\"\"\n",
        "    Clean text with unicode issues:\n",
        "    - Normalize unicode characters (√© ‚Üí e, √± ‚Üí n)\n",
        "    - Remove or replace emojis\n",
        "    - Handle special quote characters (\"\" ‚Üí \", '' ‚Üí ')\n",
        "    - Remove zero-width spaces and other invisible characters\n",
        "\n",
        "    Args:\n",
        "        text: Input text with unicode issues\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    import unicodedata\n",
        "\n",
        "    # TODO: Normalize unicode (NFD or NFC)\n",
        "    # TODO: Remove or replace emojis\n",
        "    # TODO: Normalize quotes and special characters\n",
        "    # TODO: Remove invisible characters\n",
        "\n",
        "    return text\n",
        "\n",
        "def handle_mixed_encoding(text):\n",
        "    \"\"\"\n",
        "    Handle text that may have encoding issues (latin-1, utf-8, etc.)\n",
        "    - Try to decode with different encodings\n",
        "    - Replace problematic characters with approximations\n",
        "    - Handle encoding errors gracefully\n",
        "\n",
        "    Args:\n",
        "        text: Potentially corrupted text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text with proper encoding\n",
        "    \"\"\"\n",
        "    # TODO: Handle encoding errors\n",
        "    # TODO: Replace problematic characters\n",
        "    # Hint: Use .encode() and .decode() with error handling\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie \"Inception\" was amazing! üé¨\n",
        "    It's a sci-fi masterpiece with great actors.\n",
        "    The director's name is Christopher Nolan.\n",
        "    Rating: 9/10 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Clean unicode and test\n",
        "# cleaned = clean_unicode_text(test_text)\n",
        "# print(\"Cleaned:\", cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akhpowbWNg9v"
      },
      "outputs": [],
      "source": [
        "# Exercise 6b: Handle Inconsistent Capitalization\n",
        "\n",
        "def smart_lowercase(text, preserve_acronyms=True, preserve_proper_nouns=False):\n",
        "    \"\"\"\n",
        "    Convert text to lowercase intelligently:\n",
        "    - Option 1: Preserve acronyms (NASA, AI, USA ‚Üí keep uppercase)\n",
        "    - Option 2: Preserve proper nouns (names, places) ‚Üí more complex!\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        preserve_acronyms: If True, keep acronyms uppercase\n",
        "        preserve_proper_nouns: If True, try to preserve proper nouns (challenging!)\n",
        "\n",
        "    Returns:\n",
        "        str: Smartly lowercased text\n",
        "    \"\"\"\n",
        "    # TODO: If preserve_acronyms, detect acronyms (all caps, 2+ chars)\n",
        "    # TODO: Convert rest to lowercase\n",
        "    # TODO: If preserve_proper_nouns, use heuristics (capitalized words at sentence start)\n",
        "    # Note: Full proper noun detection requires NER (Named Entity Recognition) - not covered here!\n",
        "\n",
        "    return text\n",
        "\n",
        "def handle_title_case(text):\n",
        "    \"\"\"\n",
        "    Normalize title case inconsistencies.\n",
        "    - \"Star Wars\" vs \"STAR WARS\" vs \"star wars\" ‚Üí \"star wars\"\n",
        "    - But preserve intentional capitalization when needed\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    # TODO: Handle different capitalization styles\n",
        "    # TODO: Convert to consistent lowercase (or preserve known proper nouns)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie AI is about artificial intelligence.\n",
        "    NASA scientists worked on the film.\n",
        "    The director is Christopher Nolan, not CHRIS NOLAN.\n",
        "    The film STAR WARS is a classic.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test smart lowercase\n",
        "# smart_lower = smart_lowercase(test_text, preserve_acronyms=True)\n",
        "# print(\"Smart lowercase:\", smart_lower)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy47e_KZNg9w"
      },
      "outputs": [],
      "source": [
        "# Exercise 6c: Handle Missing and Corrupted Data\n",
        "\n",
        "def preprocess_robust(text):\n",
        "    \"\"\"\n",
        "    Robust preprocessing that handles:\n",
        "    - None/NaN values\n",
        "    - Empty strings\n",
        "    - Whitespace-only strings\n",
        "    - Very long strings (truncate if needed)\n",
        "    - Non-string types (convert to string)\n",
        "\n",
        "    Args:\n",
        "        text: Potentially problematic input\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text or empty string if invalid\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # TODO: Check if text is None or NaN\n",
        "    # TODO: Check if text is empty or whitespace-only\n",
        "    # TODO: Convert to string if not already\n",
        "    # TODO: Handle edge cases (too long, wrong type, etc.)\n",
        "\n",
        "    if text is None or (isinstance(text, float) and np.isnan(text)):\n",
        "        return \"\"\n",
        "\n",
        "    # TODO: Continue with cleaning...\n",
        "\n",
        "    return str(text) if text else \"\"\n",
        "\n",
        "def batch_preprocess_robust(texts):\n",
        "    \"\"\"\n",
        "    Preprocess a list of texts, handling missing/corrupted entries.\n",
        "\n",
        "    Args:\n",
        "        texts: List of texts (may contain None, NaN, etc.)\n",
        "\n",
        "    Returns:\n",
        "        list: List of cleaned texts (same length, invalid entries become empty strings)\n",
        "    \"\"\"\n",
        "    # TODO: Process each text with preprocess_robust\n",
        "    # TODO: Maintain same length as input\n",
        "    # TODO: Log or track which entries were invalid\n",
        "\n",
        "    cleaned = []\n",
        "    invalid_indices = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        cleaned_text = preprocess_robust(text)\n",
        "        if not cleaned_text:\n",
        "            invalid_indices.append(i)\n",
        "        cleaned.append(cleaned_text)\n",
        "\n",
        "    if invalid_indices:\n",
        "        print(f\"Warning: {len(invalid_indices)} invalid entries found at indices: {invalid_indices[:10]}...\")\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# Test cases\n",
        "test_texts = [\n",
        "    \"Normal text here\",\n",
        "    None,\n",
        "    \"\",\n",
        "    \"   \",  # whitespace only\n",
        "    \"Valid text with content\",\n",
        "    float('nan'),\n",
        "    \"Another valid entry\",\n",
        "    12345,  # number instead of string\n",
        "    \"Good text\"\n",
        "]\n",
        "\n",
        "# TODO: Test robust preprocessing\n",
        "# cleaned_texts = batch_preprocess_robust(test_texts)\n",
        "# print(\"Cleaned texts:\", cleaned_texts)\n",
        "# print(f\"Valid entries: {sum(1 for t in cleaned_texts if t)}/{len(cleaned_texts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQPjQAtvNg9w"
      },
      "outputs": [],
      "source": [
        "# Exercise 6d: Handle Numbers and Units in Text\n",
        "\n",
        "def normalize_numbers_and_units(text):\n",
        "    \"\"\"\n",
        "    Normalize numbers and units for better text processing:\n",
        "    - \"100 years\" ‚Üí \"100_years\" or \"[NUMBER] years\" (preserve context)\n",
        "    - \"50%\" ‚Üí \"50_percent\" or \"[PERCENTAGE]\"\n",
        "    - \"3.5 stars\" ‚Üí \"3.5_stars\" or \"[RATING]\"\n",
        "\n",
        "    Options:\n",
        "    1. Replace with placeholders: \"[NUMBER]\", \"[PERCENTAGE]\", etc.\n",
        "    2. Keep as-is but mark: \"100_years\" (replace space with underscore)\n",
        "    3. Remove entirely: \"100 years\" ‚Üí \"years\"\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Text with normalized numbers/units\n",
        "    \"\"\"\n",
        "    # TODO: Detect numbers with units (years, dollars, percent, etc.)\n",
        "    # TODO: Normalize format (choose one approach above)\n",
        "    # TODO: Handle different number formats (integers, decimals, percentages)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_numeric_metadata(text):\n",
        "    \"\"\"\n",
        "    Extract numeric metadata (ratings, years, amounts) and store separately.\n",
        "    This allows keeping text clean while preserving important numeric information.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'text': cleaned text (numbers removed or replaced),\n",
        "            'ratings': [list of ratings],\n",
        "            'years': [list of years],\n",
        "            'amounts': [list of monetary amounts],\n",
        "            'percentages': [list of percentages]\n",
        "        }\n",
        "    \"\"\"\n",
        "    # TODO: Extract different types of numbers\n",
        "    # TODO: Remove or replace them in text\n",
        "    # TODO: Return both cleaned text and extracted metadata\n",
        "\n",
        "    metadata = {\n",
        "        'text': text,\n",
        "        'ratings': [],\n",
        "        'years': [],\n",
        "        'amounts': [],\n",
        "        'percentages': []\n",
        "    }\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# Test cases\n",
        "test_text = \"\"\"\n",
        "    The movie was released in 2010 and grossed $800 million.\n",
        "    It has a rating of 8.7/10 with 95% positive reviews.\n",
        "    The runtime is 148 minutes and it won 4 Oscars.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test number normalization\n",
        "# normalized = normalize_numbers_and_units(test_text)\n",
        "# metadata = extract_numeric_metadata(test_text)\n",
        "# print(\"Normalized:\", normalized)\n",
        "# print(\"Metadata:\", metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQnCqXyONg9x"
      },
      "outputs": [],
      "source": [
        "# Exercise 8e: Complete Robust Preprocessing Pipeline\n",
        "\n",
        "def preprocess_robust_pipeline(text,\n",
        "                               handle_unicode=True,\n",
        "                               handle_capitalization=True,\n",
        "                               handle_numbers=True,\n",
        "                               handle_missing=True):\n",
        "    \"\"\"\n",
        "    Complete robust preprocessing pipeline that handles all edge cases.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Handle missing/corrupted data\n",
        "    2. Handle unicode and special characters\n",
        "    3. Handle capitalization (smart lowercase)\n",
        "    4. Handle numbers and units (normalize or extract)\n",
        "    5. Basic cleaning (URLs, emails, etc. - from Exercise 1)\n",
        "    6. Normalize whitespace\n",
        "\n",
        "    Args:\n",
        "        text: Raw input text\n",
        "        handle_unicode: Whether to clean unicode\n",
        "        handle_capitalization: Whether to apply smart lowercase\n",
        "        handle_numbers: Whether to normalize numbers/units\n",
        "        handle_missing: Whether to handle missing data\n",
        "\n",
        "    Returns:\n",
        "        str: Fully preprocessed text\n",
        "    \"\"\"\n",
        "    # TODO: Step 1: Handle missing data\n",
        "    if handle_missing:\n",
        "        text = preprocess_robust(text)\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "    # TODO: Step 2: Handle unicode\n",
        "    if handle_unicode:\n",
        "        text = clean_unicode_text(text)\n",
        "\n",
        "    # TODO: Step 3: Handle capitalization\n",
        "    if handle_capitalization:\n",
        "        text = smart_lowercase(text, preserve_acronyms=True)\n",
        "\n",
        "    # TODO: Step 4: Handle numbers (optional - normalize or extract)\n",
        "    if handle_numbers:\n",
        "        # Option: Normalize or extract metadata\n",
        "        text = normalize_numbers_and_units(text)\n",
        "\n",
        "    # TODO: Step 5: Basic cleaning (from Exercise 1)\n",
        "    # text = clean_text(text)  # Use your function from Exercise 1\n",
        "\n",
        "    # TODO: Step 6: Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test on real movie data\n",
        "print(\"Testing robust preprocessing on movie descriptions:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test on a few movie descriptions (handle potential missing data)\n",
        "for idx in range(min(5, len(df))):\n",
        "    original = df.loc[idx, 'description']\n",
        "    if pd.isna(original):\n",
        "        print(f\"\\nMovie {idx}: [MISSING DATA]\")\n",
        "        continue\n",
        "\n",
        "    processed = preprocess_robust_pipeline(original)\n",
        "\n",
        "    print(f\"\\nMovie {idx}:\")\n",
        "    print(f\"Original (first 100 chars): {str(original)[:100]}...\")\n",
        "    print(f\"Processed (first 100 chars): {processed[:100]}...\")\n",
        "    print(f\"Length: {len(str(original))} ‚Üí {len(processed)} chars\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üí° Key Insight: Robust preprocessing handles real-world data issues!\")\n",
        "print(\"   Always test your preprocessing on actual data to find edge cases.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}